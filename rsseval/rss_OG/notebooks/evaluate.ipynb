{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the metrics In and Out of Distribution for SDDOIA & Co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from utils.train import convert_to_categories, compute_coverage, compute_coverage_hard\n",
    "from datasets.boia import BOIA\n",
    "from datasets.sddoia import SDDOIA\n",
    "from datasets.minikandinsky import MiniKandinsky\n",
    "from datasets.kandinsky import Kandinsky\n",
    "from datasets.shortcutmnist import SHORTMNIST\n",
    "from datasets.clipkandinsky import CLIPKandinsky\n",
    "from datasets.clipshortcutmnist import CLIPSHORTMNIST\n",
    "from datasets.clipboia import CLIPBOIA\n",
    "from datasets.clipSDDOIA import CLIPSDDOIA\n",
    "from models.boiadpl import BoiaDPL\n",
    "from models.SDDOIAdpl import SDDOIADPL\n",
    "from models.boialtn import BOIALTN\n",
    "from models.SDDOIAltn import SDDOIALTN\n",
    "from models.boiann import BOIAnn\n",
    "from models.SDDOIAnn import SDDOIAnn\n",
    "from models.SDDOIAcbm import SDDOIACBM\n",
    "from models.boiacbm import BoiaCBM\n",
    "from models.mnistcbm import MnistCBM\n",
    "from models.mnistdpl import MnistDPL\n",
    "from models.mnistltn import MnistLTN\n",
    "from models.mnistnn import MNISTnn\n",
    "from models.minikanddpl import MiniKandDPL\n",
    "from models.kanddpl import KandDPL\n",
    "from models.kandcbm import KandCBM\n",
    "from models.kandltn import KANDltn\n",
    "from models.kandnn import KANDnn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from argparse import Namespace\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class containing all the metrics which we are evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(\n",
    "        self,\n",
    "        concept_accuracy,\n",
    "        label_accuracy,\n",
    "        concept_f1_macro,\n",
    "        concept_f1_micro,\n",
    "        concept_f1_weighted,\n",
    "        label_f1_macro,\n",
    "        label_f1_micro,\n",
    "        label_f1_weighted,\n",
    "        collapse,\n",
    "        collapse_hard,\n",
    "        avg_nll,\n",
    "    ):\n",
    "        self.concept_accuracy = concept_accuracy\n",
    "        self.label_accuracy = label_accuracy\n",
    "        self.concept_f1_macro = concept_f1_macro\n",
    "        self.concept_f1_micro = concept_f1_micro\n",
    "        self.concept_f1_weighted = concept_f1_weighted\n",
    "        self.label_f1_macro = label_f1_macro\n",
    "        self.label_f1_micro = label_f1_micro\n",
    "        self.label_f1_weighted = label_f1_weighted\n",
    "        self.collapse = collapse\n",
    "        self.collapse_hard = collapse_hard\n",
    "        self.avg_nll = avg_nll\n",
    "\n",
    "\n",
    "class BOIAMetrics(Metrics):\n",
    "    def __init__(\n",
    "        self,\n",
    "        concept_accuracy,\n",
    "        label_accuracy,\n",
    "        concept_f1_macro,\n",
    "        concept_f1_micro,\n",
    "        concept_f1_weighted,\n",
    "        label_f1_macro,\n",
    "        label_f1_micro,\n",
    "        label_f1_weighted,\n",
    "        collapse,\n",
    "        collapse_hard,\n",
    "        collapse_forward,\n",
    "        collapse_stop,\n",
    "        collapse_left,\n",
    "        collapse_right,\n",
    "        collapse_hard_forward,\n",
    "        collapse_hard_stop,\n",
    "        collapse_hard_left,\n",
    "        collapse_hard_right,\n",
    "        mean_collapse,\n",
    "        mean_hard_collapse,\n",
    "        avg_nll,\n",
    "    ):\n",
    "        super(BOIAMetrics, self).__init__(\n",
    "            concept_accuracy,\n",
    "            label_accuracy,\n",
    "            concept_f1_macro,\n",
    "            concept_f1_micro,\n",
    "            concept_f1_weighted,\n",
    "            label_f1_macro,\n",
    "            label_f1_micro,\n",
    "            label_f1_weighted,\n",
    "            collapse,\n",
    "            collapse_hard,\n",
    "            avg_nll,\n",
    "        )\n",
    "        self.collapse_forward = collapse_forward\n",
    "        self.collapse_stop = collapse_stop\n",
    "        self.collapse_left = collapse_left\n",
    "        self.collapse_right = collapse_right\n",
    "        self.collapse_hard_forward = collapse_hard_forward\n",
    "        self.collapse_hard_stop = collapse_hard_stop\n",
    "        self.collapse_hard_left = collapse_hard_left\n",
    "        self.collapse_hard_right = collapse_hard_right\n",
    "        self.mean_collapse = mean_collapse\n",
    "        self.mean_hard_collapse = mean_hard_collapse\n",
    "\n",
    "\n",
    "class KandMetrics(Metrics):\n",
    "    def __init__(\n",
    "        self,\n",
    "        concept_accuracy,\n",
    "        label_accuracy,\n",
    "        concept_f1_macro,\n",
    "        concept_f1_micro,\n",
    "        concept_f1_weighted,\n",
    "        label_f1_macro,\n",
    "        label_f1_micro,\n",
    "        label_f1_weighted,\n",
    "        collapse,\n",
    "        collapse_hard,\n",
    "        avg_nll,\n",
    "        collapse_shapes,\n",
    "        collapse_hard_shapes,\n",
    "        collapse_color,\n",
    "        collapse_hard_color,\n",
    "        mean_collapse,\n",
    "        mean_collapse_hard,\n",
    "    ):\n",
    "        super(KandMetrics, self).__init__(\n",
    "            concept_accuracy,\n",
    "            label_accuracy,\n",
    "            concept_f1_macro,\n",
    "            concept_f1_micro,\n",
    "            concept_f1_weighted,\n",
    "            label_f1_macro,\n",
    "            label_f1_micro,\n",
    "            label_f1_weighted,\n",
    "            collapse,\n",
    "            collapse_hard,\n",
    "            avg_nll,\n",
    "        )\n",
    "        self.collapse_shapes = collapse_shapes\n",
    "        self.collapse_hard_shapes = collapse_hard_shapes\n",
    "        self.collapse_color = collapse_color\n",
    "        self.collapse_hard_color = collapse_hard_color\n",
    "        self.mean_collapse = mean_collapse\n",
    "        self.mean_collapse_hard = mean_collapse_hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function used to compute the concept collapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_concept_collapse(true_concepts, predicted_concepts, multilabel=False):\n",
    "    if multilabel:\n",
    "        true_concepts = convert_to_categories(true_concepts.astype(int))\n",
    "        predicted_concepts = convert_to_categories(predicted_concepts.astype(int))\n",
    "\n",
    "    return 1 - compute_coverage(confusion_matrix(true_concepts, predicted_concepts))\n",
    "\n",
    "\n",
    "def compute_hard_concept_collapse(true_concepts, predicted_concepts, multilabel=False):\n",
    "    if multilabel:\n",
    "        true_concepts = convert_to_categories(true_concepts.astype(int))\n",
    "        predicted_concepts = convert_to_categories(predicted_concepts.astype(int))\n",
    "\n",
    "    return 1 - compute_coverage_hard(\n",
    "        confusion_matrix(true_concepts, predicted_concepts)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function used to plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(\n",
    "    true_labels,\n",
    "    predicted_labels,\n",
    "    classes,\n",
    "    normalize=False,\n",
    "    title=None,\n",
    "    is_boia=False,\n",
    "    cmap=plt.cm.Oranges,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    cm = np.zeros((len(classes), len(classes)))\n",
    "    for i in range(len(true_labels)):\n",
    "        cm[true_labels[i], predicted_labels[i]] += 1\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\")\n",
    "        row_sums = cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm = np.where(row_sums == 0, 0, cm / row_sums)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.set(font_scale=1.8)\n",
    "    red_yellow_palette = sns.color_palette(\"OrRd\", as_cmap=True)\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=False,\n",
    "        fmt=\".2f\" if normalize else \"d\",\n",
    "        cmap=red_yellow_palette,\n",
    "        cbar=True,\n",
    "        xticklabels=classes,\n",
    "        yticklabels=classes,\n",
    "    )\n",
    "    if title:\n",
    "        plt.savefig(title, format=\"pdf\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function used to compute the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    true_labels,\n",
    "    predicted_labels,\n",
    "    true_concepts,\n",
    "    predicted_concepts,\n",
    "    avg_nll,\n",
    "    dataset_name,\n",
    "    model_name,\n",
    "    seed,\n",
    "):\n",
    "\n",
    "    # multilabel or not\n",
    "    multilabel_concept = False\n",
    "    multilabel_label = False\n",
    "\n",
    "    if dataset_name in [\"boia\", \"sddoia\", \"clipboia\", \"clipSDDOIA\"]:\n",
    "        multilabel_concept = True\n",
    "        multilabel_label = True\n",
    "\n",
    "    if dataset_name in [\"kandinsky\", \"minikandinsky\", \"clipkandinsky\"]:\n",
    "        collapse_true_concepts_list = torch.tensor(true_concepts)\n",
    "        collapse_true_concepts_list = torch.split(collapse_true_concepts_list, 3, dim=1)\n",
    "        collapse_pred_concepts_list = torch.tensor(predicted_concepts)\n",
    "        collapse_pred_concepts_list = torch.split(collapse_pred_concepts_list, 3, dim=1)\n",
    "\n",
    "        collapse_true_concepts_1 = collapse_true_concepts_list[0].flatten()\n",
    "        collapse_true_concepts_2 = collapse_true_concepts_list[1].flatten()\n",
    "        collapse_true_concepts = torch.stack(\n",
    "            (collapse_true_concepts_1, collapse_true_concepts_2), dim=1\n",
    "        )\n",
    "        # to int\n",
    "        collapse_true_concepts = (\n",
    "            collapse_true_concepts[:, 0] * 3 + collapse_true_concepts[:, 1]\n",
    "        )\n",
    "        collapse_true_concepts = collapse_true_concepts.detach().numpy()\n",
    "\n",
    "        collapse_pred_concepts_1 = collapse_pred_concepts_list[0].flatten()\n",
    "        collapse_pred_concepts_2 = collapse_pred_concepts_list[1].flatten()\n",
    "        collapse_pred_concepts = torch.stack(\n",
    "            (collapse_pred_concepts_1, collapse_pred_concepts_2), dim=1\n",
    "        )\n",
    "        # to int\n",
    "        collapse_pred_concepts = (\n",
    "            collapse_pred_concepts[:, 0] * 3 + collapse_pred_concepts[:, 1]\n",
    "        )\n",
    "        collapse_pred_concepts = collapse_pred_concepts.detach().numpy()\n",
    "\n",
    "        # total collapse\n",
    "        collapse = compute_concept_collapse(\n",
    "            collapse_true_concepts, collapse_pred_concepts, multilabel_concept\n",
    "        )\n",
    "\n",
    "        collapse_hard = compute_hard_concept_collapse(\n",
    "            collapse_true_concepts, collapse_pred_concepts, multilabel_concept\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # total collapse\n",
    "        collapse = compute_concept_collapse(\n",
    "            true_concepts, predicted_concepts, multilabel_concept\n",
    "        )\n",
    "\n",
    "        collapse_hard = compute_hard_concept_collapse(\n",
    "            true_concepts, predicted_concepts, multilabel_concept\n",
    "        )\n",
    "\n",
    "    if dataset_name in [\"boia\", \"sddoia\", \"clipboia\", \"clipSDDOIA\"]:\n",
    "        # additional metrics for boia and sddoia\n",
    "        collapse_forward, collapse_hard_forward = compute_concept_collapse(\n",
    "            true_concepts[:, :3], predicted_concepts[:, :3], True\n",
    "        ), compute_hard_concept_collapse(\n",
    "            true_concepts[:, :3], predicted_concepts[:, :3], True\n",
    "        )\n",
    "        collapse_stop, collapse_hard_stop = compute_concept_collapse(\n",
    "            true_concepts[:, 3:9], predicted_concepts[:, 3:9], True\n",
    "        ), compute_hard_concept_collapse(\n",
    "            true_concepts[:, 3:9], predicted_concepts[:, 3:9], True\n",
    "        )\n",
    "        collapse_left, collapse_hard_left = compute_concept_collapse(\n",
    "            true_concepts[:, 9:15], predicted_concepts[:, 9:15], True\n",
    "        ), compute_hard_concept_collapse(\n",
    "            true_concepts[:, 9:15], predicted_concepts[:, 9:15], True\n",
    "        )\n",
    "        collapse_right, collapse_hard_right = compute_concept_collapse(\n",
    "            true_concepts[:, 15:21], predicted_concepts[:, 15:21], True\n",
    "        ), compute_hard_concept_collapse(\n",
    "            true_concepts[:, 15:21], predicted_concepts[:, 15:21], True\n",
    "        )\n",
    "\n",
    "        mean_collapse, mean_hard_collapse = np.mean(\n",
    "            [collapse_forward, collapse_stop, collapse_left, collapse_right]\n",
    "        ), np.mean(\n",
    "            [\n",
    "                collapse_hard_forward,\n",
    "                collapse_hard_stop,\n",
    "                collapse_hard_left,\n",
    "                collapse_hard_right,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    elif dataset_name in [\"minikandinsky\", \"kandinsky\", \"clipkandinsky\"]:\n",
    "        # additional metrics for boia and sddoia\n",
    "        collapse_color, collapse_hard_color = compute_concept_collapse(\n",
    "            true_concepts[:, 3:6].reshape(-1),\n",
    "            predicted_concepts[:, 3:6].reshape(-1),\n",
    "            False,\n",
    "        ), compute_hard_concept_collapse(\n",
    "            true_concepts[:, 3:6].reshape(-1),\n",
    "            predicted_concepts[:, 3:6].reshape(-1),\n",
    "            False,\n",
    "        )\n",
    "        collapse_shapes, collapse_hard_shapes = compute_concept_collapse(\n",
    "            true_concepts[:, :3].reshape(-1),\n",
    "            predicted_concepts[:, :3].reshape(-1),\n",
    "            False,\n",
    "        ), compute_hard_concept_collapse(\n",
    "            true_concepts[:, :3].reshape(-1),\n",
    "            predicted_concepts[:, :3].reshape(-1),\n",
    "            False,\n",
    "        )\n",
    "\n",
    "        mean_collapse, mean_collapse_hard = np.mean(\n",
    "            [collapse_color, collapse_shapes]\n",
    "        ), np.mean([collapse_hard_color, collapse_hard_shapes])\n",
    "\n",
    "    if multilabel_concept:\n",
    "        concept_accuracy, concept_f1_macro, concept_f1_micro, concept_f1_weighted = (\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        for i in range(true_concepts.shape[1]):\n",
    "            concept_accuracy += accuracy_score(true_concepts[i], predicted_concepts[i])\n",
    "            concept_f1_macro += f1_score(\n",
    "                true_concepts[i], predicted_concepts[i], average=\"macro\"\n",
    "            )\n",
    "            concept_f1_micro += f1_score(\n",
    "                true_concepts[i], predicted_concepts[i], average=\"micro\"\n",
    "            )\n",
    "            concept_f1_weighted += f1_score(\n",
    "                true_concepts[i], predicted_concepts[i], average=\"weighted\"\n",
    "            )\n",
    "\n",
    "        concept_accuracy = concept_accuracy / true_concepts.shape[1]\n",
    "        concept_f1_macro = concept_f1_macro / true_concepts.shape[1]\n",
    "        concept_f1_micro = concept_f1_micro / true_concepts.shape[1]\n",
    "        concept_f1_weighted = concept_f1_weighted / true_concepts.shape[1]\n",
    "\n",
    "        label_accuracy, label_f1_macro, label_f1_micro, label_f1_weighted = 0, 0, 0, 0\n",
    "    elif dataset_name in [\"kandinsky\", \"minikandinsky\", \"clipkandinsky\"]:\n",
    "        concept_accuracy_color = accuracy_score(\n",
    "            true_concepts[:, 3:6].reshape(-1), predicted_concepts[:, 3:6].reshape(-1)\n",
    "        )\n",
    "        concept_f1_macro_color = f1_score(\n",
    "            true_concepts[:, 3:6].reshape(-1),\n",
    "            predicted_concepts[:, 3:6].reshape(-1),\n",
    "            average=\"macro\",\n",
    "        )\n",
    "        concept_f1_micro_color = f1_score(\n",
    "            true_concepts[:, 3:6].reshape(-1),\n",
    "            predicted_concepts[:, 3:6].reshape(-1),\n",
    "            average=\"micro\",\n",
    "        )\n",
    "        concept_f1_weighted_color = f1_score(\n",
    "            true_concepts[:, 3:6].reshape(-1),\n",
    "            predicted_concepts[:, 3:6].reshape(-1),\n",
    "            average=\"weighted\",\n",
    "        )\n",
    "\n",
    "        concept_accuracy_shape = accuracy_score(\n",
    "            true_concepts[:, :3].reshape(-1), predicted_concepts[:, :3].reshape(-1)\n",
    "        )\n",
    "        concept_f1_macro_shape = f1_score(\n",
    "            true_concepts[:, :3].reshape(-1),\n",
    "            predicted_concepts[:, :3].reshape(-1),\n",
    "            average=\"macro\",\n",
    "        )\n",
    "        concept_f1_micro_shape = f1_score(\n",
    "            true_concepts[:, :3].reshape(-1),\n",
    "            predicted_concepts[:, :3].reshape(-1),\n",
    "            average=\"micro\",\n",
    "        )\n",
    "        concept_f1_weighted_shape = f1_score(\n",
    "            true_concepts[:, :3].reshape(-1),\n",
    "            predicted_concepts[:, :3].reshape(-1),\n",
    "            average=\"weighted\",\n",
    "        )\n",
    "\n",
    "        concept_accuracy = np.mean([concept_accuracy_color, concept_accuracy_shape])\n",
    "        concept_f1_macro = np.mean([concept_f1_macro_color, concept_f1_macro_shape])\n",
    "        concept_f1_micro = np.mean([concept_f1_micro_color, concept_f1_micro_shape])\n",
    "        concept_f1_weighted = np.mean(\n",
    "            [concept_f1_weighted_color, concept_f1_weighted_shape]\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        concept_accuracy = accuracy_score(true_concepts, predicted_concepts)\n",
    "        concept_f1_macro = f1_score(true_concepts, predicted_concepts, average=\"macro\")\n",
    "        concept_f1_micro = f1_score(true_concepts, predicted_concepts, average=\"micro\")\n",
    "        concept_f1_weighted = f1_score(\n",
    "            true_concepts, predicted_concepts, average=\"weighted\"\n",
    "        )\n",
    "\n",
    "    if multilabel_label:\n",
    "        for i in range(true_labels.shape[1]):\n",
    "            label_accuracy += accuracy_score(true_labels[i], predicted_labels[i])\n",
    "            label_f1_macro += f1_score(\n",
    "                true_labels[i], predicted_labels[i], average=\"macro\"\n",
    "            )\n",
    "            label_f1_micro += f1_score(\n",
    "                true_labels[i], predicted_labels[i], average=\"micro\"\n",
    "            )\n",
    "            label_f1_weighted += f1_score(\n",
    "                true_labels[i], predicted_labels[i], average=\"weighted\"\n",
    "            )\n",
    "\n",
    "        label_accuracy = label_accuracy / true_labels.shape[1]\n",
    "        label_f1_macro = label_f1_macro / true_labels.shape[1]\n",
    "        label_f1_micro = label_f1_micro / true_labels.shape[1]\n",
    "        label_f1_weighted = label_f1_weighted / true_labels.shape[1]\n",
    "    else:\n",
    "        label_accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "        label_f1_macro = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
    "        label_f1_micro = f1_score(true_labels, predicted_labels, average=\"micro\")\n",
    "        label_f1_weighted = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
    "\n",
    "    if dataset_name in [\"boia\", \"sddoia\", \"clipboia\", \"clipSDDOIA\"]:\n",
    "        metrics = BOIAMetrics(\n",
    "            concept_accuracy=concept_accuracy,\n",
    "            label_accuracy=label_accuracy,\n",
    "            concept_f1_macro=concept_f1_macro,\n",
    "            concept_f1_micro=concept_f1_micro,\n",
    "            concept_f1_weighted=concept_f1_weighted,\n",
    "            label_f1_macro=label_f1_macro,\n",
    "            label_f1_micro=label_f1_micro,\n",
    "            label_f1_weighted=label_f1_weighted,\n",
    "            collapse=collapse,\n",
    "            collapse_hard=collapse_hard,\n",
    "            collapse_forward=collapse_forward,\n",
    "            collapse_stop=collapse_stop,\n",
    "            collapse_right=collapse_right,\n",
    "            collapse_left=collapse_left,\n",
    "            collapse_hard_forward=collapse_hard_forward,\n",
    "            collapse_hard_stop=collapse_hard_stop,\n",
    "            collapse_hard_right=collapse_hard_right,\n",
    "            collapse_hard_left=collapse_hard_left,\n",
    "            mean_collapse=mean_collapse,\n",
    "            mean_hard_collapse=mean_hard_collapse,\n",
    "            avg_nll=avg_nll,\n",
    "        )\n",
    "    elif dataset_name in [\"minikandinsky\", \"kandinsky\", \"clipkandinsky\"]:\n",
    "        metrics = KandMetrics(\n",
    "            concept_accuracy=concept_accuracy,\n",
    "            label_accuracy=label_accuracy,\n",
    "            concept_f1_macro=concept_f1_macro,\n",
    "            concept_f1_micro=concept_f1_micro,\n",
    "            concept_f1_weighted=concept_f1_weighted,\n",
    "            label_f1_macro=label_f1_macro,\n",
    "            label_f1_micro=label_f1_micro,\n",
    "            label_f1_weighted=label_f1_weighted,\n",
    "            collapse=collapse,\n",
    "            collapse_hard=collapse_hard,\n",
    "            avg_nll=avg_nll,\n",
    "            collapse_shapes=collapse_shapes,\n",
    "            collapse_color=collapse_color,\n",
    "            collapse_hard_shapes=collapse_hard_shapes,\n",
    "            mean_collapse_hard=mean_collapse_hard,\n",
    "            mean_collapse=mean_collapse,\n",
    "            collapse_hard_color=collapse_hard_color,\n",
    "        )\n",
    "    else:\n",
    "        metrics = Metrics(\n",
    "            concept_accuracy=concept_accuracy,\n",
    "            label_accuracy=label_accuracy,\n",
    "            concept_f1_macro=concept_f1_macro,\n",
    "            concept_f1_micro=concept_f1_micro,\n",
    "            concept_f1_weighted=concept_f1_weighted,\n",
    "            label_f1_macro=label_f1_macro,\n",
    "            label_f1_micro=label_f1_micro,\n",
    "            label_f1_weighted=label_f1_weighted,\n",
    "            collapse=collapse,\n",
    "            collapse_hard=collapse_hard,\n",
    "            avg_nll=avg_nll,\n",
    "        )\n",
    "\n",
    "    if dataset_name in [\"shortmnist\"]:\n",
    "        plot_confusion_matrix(\n",
    "            true_concepts,\n",
    "            predicted_concepts,\n",
    "            classes=[i for i in range(10)],\n",
    "            normalize=True,\n",
    "            title=f\"{model_name}_{dataset_name}_{seed}.pdf\",\n",
    "            is_boia=True,\n",
    "        )\n",
    "    elif dataset_name in [\"boia\", \"sddoia\"]:\n",
    "\n",
    "        plot_confusion_matrix(\n",
    "            convert_to_categories(true_concepts[:, :3].astype(int)),\n",
    "            convert_to_categories(predicted_concepts[:, :3].astype(int)),\n",
    "            [\"\" for i in range(2**3)],\n",
    "            True,\n",
    "            f\"{model_name}_{dataset_name}_{seed}_forward.pdf\",\n",
    "        )\n",
    "        plot_confusion_matrix(\n",
    "            convert_to_categories(true_concepts[:, 3:9].astype(int)),\n",
    "            convert_to_categories(predicted_concepts[:, 3:9].astype(int)),\n",
    "            [\"\" for i in range(2**6)],\n",
    "            True,\n",
    "            f\"{model_name}_{dataset_name}_{seed}_stop.pdf\",\n",
    "        )\n",
    "        plot_confusion_matrix(\n",
    "            convert_to_categories(true_concepts[:, 9:15].astype(int)),\n",
    "            convert_to_categories(predicted_concepts[:, 9:15].astype(int)),\n",
    "            [\"\" for i in range(2**6)],\n",
    "            True,\n",
    "            f\"{model_name}_{dataset_name}_{seed}_left.pdf\",\n",
    "        )\n",
    "        plot_confusion_matrix(\n",
    "            convert_to_categories(true_concepts[:, 15:21].astype(int)),\n",
    "            convert_to_categories(predicted_concepts[:, 15:21].astype(int)),\n",
    "            [\"\" for i in range(2**6)],\n",
    "            True,\n",
    "            f\"{model_name}_{dataset_name}_{seed}_right.pdf\",\n",
    "        )\n",
    "    elif dataset_name in [\"kandinsky\", \"minikandinsky\"]:\n",
    "        plot_confusion_matrix(\n",
    "            true_concepts,\n",
    "            predicted_concepts,\n",
    "            classes=[i for i in range(10)],\n",
    "            normalize=True,\n",
    "            title=f\"{model_name}_{dataset_name}_{seed}.pdf\",\n",
    "        )\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the right dataset and the right model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(datasetname, args):\n",
    "    if datasetname.lower() == \"boia\":\n",
    "        return BOIA(args)\n",
    "    if datasetname.lower() == \"sddoia\":\n",
    "        return SDDOIA(args)\n",
    "    if datasetname.lower() == \"minikandinsky\":\n",
    "        return MiniKandinsky(args)\n",
    "    if datasetname.lower() == \"kandinsky\":\n",
    "        return Kandinsky(args)\n",
    "    if datasetname.lower() == \"shortmnist\":\n",
    "        return SHORTMNIST(args)\n",
    "    if datasetname.lower() == \"clipkandinsky\":\n",
    "        return CLIPKandinsky(args)\n",
    "    if datasetname.lower() == \"clipshortmnist\":\n",
    "        return CLIPSHORTMNIST(args)\n",
    "    if datasetname.lower() == \"clipboia\":\n",
    "        return CLIPBOIA(args)\n",
    "    if datasetname.lower() == \"clipSDDOIA\":\n",
    "        return CLIPSDDOIA(args)\n",
    "\n",
    "    raise NotImplementedError(f\"Dataset {datasetname} missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(modelname, encoder, args):\n",
    "    if modelname.lower() == \"boiadpl\":\n",
    "        return BoiaDPL(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"SDDOIAdpl\":\n",
    "        return SDDOIADPL(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"boialtn\":\n",
    "        return BOIALTN(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"SDDOIAltn\":\n",
    "        return SDDOIALTN(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"boiann\":\n",
    "        return BOIAnn(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"SDDOIAnn\":\n",
    "        return SDDOIAnn(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"boiacbm\":\n",
    "        return BoiaCBM(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"SDDOIAcbm\":\n",
    "        return SDDOIACBM(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"minikanddpl\":\n",
    "        return MiniKandDPL(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"kandltn\":\n",
    "        return KANDltn(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"kandnn\":\n",
    "        return KANDnn(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"kanddpl\":\n",
    "        return KandDPL(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"kandcbm\":\n",
    "        return KandCBM(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"mnistdpl\":\n",
    "        return MnistDPL(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"mnistltn\":\n",
    "        return MnistLTN(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"mnistnn\":\n",
    "        return MNISTnn(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"mnistcbm\":\n",
    "        return MnistCBM(encoder=encoder, args=args)\n",
    "\n",
    "    raise NotImplementedError(f\"Model {modelname} missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    backbone=\"conceptizer\",  #\n",
    "    preprocess=0,\n",
    "    finetuning=0,\n",
    "    batch_size=256,\n",
    "    n_epochs=20,\n",
    "    validate=1,\n",
    "    dataset=\"sddoia\",\n",
    "    lr=0.001,\n",
    "    exp_decay=0.99,\n",
    "    warmup_steps=1,\n",
    "    wandb=None,\n",
    "    task=\"boia\",\n",
    "    boia_model=\"ce\",\n",
    "    model=\"SDDOIAltn\",\n",
    "    c_sup=1,\n",
    "    which_c=[-1],\n",
    "    joint=False,\n",
    "    boia_ood_knowledge=True,\n",
    ")\n",
    "\n",
    "# get dataset\n",
    "dataset = get_dataset(args.dataset, args)\n",
    "# get model\n",
    "model = get_model(modelname=args.model, encoder=dataset.get_backbone()[0], args=args)\n",
    "\n",
    "# set cpu for the moment\n",
    "model.device = \"cuda:2\"\n",
    "\n",
    "model.to(model.device)\n",
    "if hasattr(model, \"encoder\"):\n",
    "    model.encoder.to(model.device)\n",
    "if hasattr(model, \"net\"):\n",
    "    model.net.to(model.device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the seeds of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [123, 456, 789, 1011, 1213, 1415, 1617, 1819, 2021, 2223]\n",
    "model_path = f\"../best_model_{args.dataset}_{args.model}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through the dataset and retrive concepts and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concepts_and_labels_boia(out_labels, out_concepts):\n",
    "    batch_size = out_labels.size(0)\n",
    "\n",
    "    predicted_labels, predicted_concepts = [], []\n",
    "\n",
    "    for idx_batch in range(batch_size):\n",
    "        prob_labels = torch.split(out_labels[idx_batch], 2)\n",
    "        prob_concepts = torch.split(out_concepts[idx_batch], 2)\n",
    "\n",
    "        tmp_lab, tmp_conc = [], []\n",
    "\n",
    "        for l_lab in prob_labels:\n",
    "            tmp_lab.append(torch.argmax(l_lab, dim=0))\n",
    "        for l_conc in prob_concepts:\n",
    "            tmp_conc.append(torch.argmax(l_conc, dim=0))\n",
    "\n",
    "        predicted_labels.append(torch.tensor([tmp_lab]))\n",
    "        predicted_concepts.append(torch.tensor([tmp_conc]))\n",
    "\n",
    "    predicted_labels = torch.concatenate(predicted_labels, dim=0)\n",
    "    predicted_concepts = torch.concatenate(predicted_concepts, dim=0)\n",
    "\n",
    "    return predicted_labels, predicted_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concepts_and_labels_mnist(\n",
    "    out_labels, out_concepts, true_concepts, is_ood=False\n",
    "):\n",
    "\n",
    "    # filtering out the extended support\n",
    "    if not is_ood:\n",
    "        for i in range(19):\n",
    "            if i in [6, 10, 12]:\n",
    "                continue\n",
    "            out_labels[:, i] = 0\n",
    "\n",
    "    predicted_labels = torch.argmax(out_labels, dim=-1)\n",
    "    predicted_concepts = torch.argmax(out_concepts, dim=-1)\n",
    "\n",
    "    predicted_concepts = predicted_concepts.view(predicted_concepts.numel())\n",
    "    refactored_true_concepts = true_concepts.view(true_concepts.numel())\n",
    "\n",
    "    return predicted_labels, predicted_concepts, refactored_true_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concepts_and_labels_kand(out_labels, out_concepts, true_concepts):\n",
    "\n",
    "    # take the prediction\n",
    "    predicted_labels = torch.argmax(out_labels, dim=1)\n",
    "\n",
    "    # stack colors and shapes on top of each other\n",
    "    refactored_true_concepts = torch.split(true_concepts, 1, dim=1)\n",
    "    refactored_true_concepts = torch.concatenate(\n",
    "        refactored_true_concepts, dim=0\n",
    "    ).squeeze(1)\n",
    "\n",
    "    # take the prediction\n",
    "    predicted_concepts_list = torch.split(out_concepts, 3, dim=2)\n",
    "    predicted_concepts = []\n",
    "    # take the argmax\n",
    "    for pc in predicted_concepts_list:\n",
    "        predicted_concepts.append(torch.argmax(pc, dim=2))\n",
    "    predicted_concepts = torch.stack(predicted_concepts, dim=2)\n",
    "\n",
    "    # make them the same dimension as the groundtruth\n",
    "    predicted_concepts = torch.split(predicted_concepts, 1, dim=1)\n",
    "    predicted_concepts = torch.concatenate(predicted_concepts, dim=0).squeeze(1)\n",
    "\n",
    "    return predicted_labels, torch.squeeze(predicted_concepts), refactored_true_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_concepts_and_labels(model, dataset, dataset_name, is_ood=False):\n",
    "\n",
    "    true_labels, predicted_labels, true_concepts, predicted_concepts = [], [], [], []\n",
    "\n",
    "    nll_loss = 0.0\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    for i, data in enumerate(tqdm(dataset)):\n",
    "        images, labels, concepts = data\n",
    "        images, labels, concepts = (\n",
    "            images.to(model.device),\n",
    "            labels.to(model.device),\n",
    "            concepts.to(model.device),\n",
    "        )\n",
    "\n",
    "        # filtering out the middle rules supervision\n",
    "        if dataset_name in [\"kandinsky\", \"minikandinsky\", \"clipkandinsky\"]:\n",
    "            labels = labels[:, -1]\n",
    "\n",
    "        out_dict = model(images)\n",
    "\n",
    "        out_label, out_concept = None, None\n",
    "\n",
    "        if dataset_name in [\"boia\", \"sddoia\", \"clipboia\", \"clipSDDOIA\"]:\n",
    "            class_predictions = torch.split(out_dict[\"YS\"], 2, dim=1)\n",
    "            assert len(class_predictions) == 4\n",
    "\n",
    "            loss = 0\n",
    "            for i, _pred in enumerate(class_predictions):\n",
    "                loss += criterion(_pred.float().cpu(), labels[:, i].long().cpu())\n",
    "            loss /= len(class_predictions)\n",
    "        else:\n",
    "            loss = criterion(out_dict[\"YS\"].float().cpu(), labels.long().cpu())\n",
    "\n",
    "        nll_loss += loss.item()\n",
    "\n",
    "        if dataset_name in [\"boia\", \"sddoia\", \"clipboia\", \"clipSDDOIA\"]:\n",
    "            out_label, out_concept = get_concepts_and_labels_boia(\n",
    "                out_dict[\"YS\"], out_dict[\"pCS\"]\n",
    "            )\n",
    "        elif dataset_name in [\"shortmnist\", \"clipshortmnist\"]:\n",
    "            out_label, out_concept, concepts = get_concepts_and_labels_mnist(\n",
    "                out_dict[\"YS\"], out_dict[\"pCS\"], concepts, is_ood\n",
    "            )\n",
    "        elif dataset_name in [\"kandinsky\", \"minikandinsky\", \"clipkandinsky\"]:\n",
    "            out_label, out_concept, concepts = get_concepts_and_labels_kand(\n",
    "                out_dict[\"YS\"], out_dict[\"pCS\"], concepts\n",
    "            )\n",
    "\n",
    "        true_labels.append(labels.cpu().numpy())\n",
    "        true_concepts.append(concepts.cpu().numpy())\n",
    "\n",
    "        predicted_labels.append(out_label.detach().cpu().numpy())\n",
    "        predicted_concepts.append(out_concept.cpu().numpy())\n",
    "\n",
    "    # concatenate\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "    predicted_labels = np.concatenate(predicted_labels, axis=0)\n",
    "    true_concepts = np.concatenate(true_concepts, axis=0)\n",
    "    predicted_concepts = np.concatenate(predicted_concepts, axis=0)\n",
    "\n",
    "    avg_nll = nll_loss / len(dataset.dataset)\n",
    "\n",
    "    assert true_labels.shape == predicted_labels.shape\n",
    "    assert true_concepts.shape == predicted_concepts.shape\n",
    "\n",
    "    return true_labels, predicted_labels, true_concepts, predicted_concepts, avg_nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model, test_set, dataset_name, model_name, ood_set=None, ood_set_2=None\n",
    "):  # TODO: define attributes\n",
    "\n",
    "    # List of metics\n",
    "    in_metrics_list = []\n",
    "    ood_metrics_list = []\n",
    "    ood_metrics_2_list = []\n",
    "\n",
    "    n_files = 0\n",
    "\n",
    "    # Loop through seeds\n",
    "    for seed in seeds:\n",
    "        print(\"Doing\", seed, \"...\")\n",
    "\n",
    "        to_add = \"\"  # \"_joint\" # \"\"\n",
    "        print(\"TO ADD\", to_add)\n",
    "\n",
    "        current_model_path = f\"{model_path}_{seed}{to_add}.pth\"\n",
    "\n",
    "        if not os.path.exists(current_model_path):\n",
    "            print(f\"{current_model_path} is missing...\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Loading {current_model_path}...\")\n",
    "\n",
    "        n_files += 1\n",
    "\n",
    "        try:\n",
    "            # retrieve the status dict\n",
    "            model_state_dict = torch.load(current_model_path)\n",
    "            # Load the model status dict\n",
    "            model.load_state_dict(model_state_dict)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        if dataset_name == \"shortmnist\":\n",
    "            model = model.float()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        ind_data = retrive_concepts_and_labels(model, test_set, dataset_name)\n",
    "\n",
    "        if ood_set is not None:\n",
    "            out_data = retrive_concepts_and_labels(\n",
    "                model, ood_set, dataset_name, is_ood=True\n",
    "            )\n",
    "\n",
    "        if ood_set_2 is not None:\n",
    "            out_data_2 = retrive_concepts_and_labels(\n",
    "                model, ood_set_2, dataset_name, is_ood=True\n",
    "            )\n",
    "\n",
    "        in_metrics = compute_metrics(*ind_data, dataset_name, model_name, seed)\n",
    "        in_metrics_list.append(in_metrics)\n",
    "\n",
    "        if ood_set is not None:\n",
    "            ood_metrics = compute_metrics(*out_data, dataset_name, model_name, seed)\n",
    "            ood_metrics_list.append(ood_metrics)\n",
    "\n",
    "        if ood_set_2 is not None:\n",
    "            ood_metrics_2 = compute_metrics(*out_data_2, dataset_name, model_name, seed)\n",
    "            ood_metrics_2_list.append(ood_metrics_2)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    assert n_files > 1, \"At least 2 files to compare\"\n",
    "\n",
    "    # Compute standard deviation for each metric\n",
    "    for key in vars(in_metrics_list[0]):  # the key are always the same\n",
    "        # skip hidden elements\n",
    "        if not key.startswith(\"_\"):\n",
    "            # retrieve the list of values\n",
    "            in_metric_values = [getattr(metrics, key) for metrics in in_metrics_list]\n",
    "            ood_metric_values = [getattr(metrics, key) for metrics in ood_metrics_list]\n",
    "            ood_metric_2_values = [\n",
    "                getattr(metrics, key) for metrics in ood_metrics_2_list\n",
    "            ]\n",
    "\n",
    "            # convert lists to NumPy arrays\n",
    "            in_metric_values_arr = np.array(in_metric_values)\n",
    "            ood_metric_values_arr = np.array(ood_metric_values)\n",
    "            ood_metric_values_2_arr = np.array(ood_metric_2_values)\n",
    "\n",
    "            # Compute the standard deviation\n",
    "            in_metric_std_dev = np.std(in_metric_values_arr)\n",
    "            ood_metric_std_dev = np.std(ood_metric_values_arr)\n",
    "            ood_metric_2_std_dev = np.std(ood_metric_values_2_arr)\n",
    "\n",
    "            # Compute the mean\n",
    "            in_metric_std_mean = np.mean(in_metric_values_arr)\n",
    "            ood_metric_std_mean = np.mean(ood_metric_values_arr)\n",
    "            ood_metric_2_std_mean = np.mean(ood_metric_values_2_arr)\n",
    "\n",
    "            print(\n",
    "                \"\\n{} (In): ${:.2f} \\pm {:.2f}$\".format(\n",
    "                    key.replace(\"_\", \" \").title(),\n",
    "                    round(in_metric_std_mean, 2),\n",
    "                    round(in_metric_std_dev, 2),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if ood_set is not None:\n",
    "                print(\n",
    "                    \"{} (OOD): ${:.2f} \\pm {:.2f}$\".format(\n",
    "                        key.replace(\"_\", \" \").title(),\n",
    "                        round(ood_metric_std_mean, 2),\n",
    "                        round(ood_metric_std_dev, 2),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            if ood_set_2 is not None:\n",
    "                print(\n",
    "                    \"{} (OOD 2): ${:.2f} \\pm {:.2f}$\".format(\n",
    "                        key.replace(\"_\", \" \").title(),\n",
    "                        round(ood_metric_2_std_mean, 2),\n",
    "                        round(ood_metric_2_std_dev, 2),\n",
    "                    )\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run all the things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get loaders\n",
    "train_loader, val_loader, test_loader = dataset.get_data_loaders()\n",
    "# Get ood set if it exists\n",
    "ood_loader = getattr(dataset, \"ood_loader_ambulance\", None)\n",
    "# ood_ambulance = getattr(dataset, \"ood_loader_2\", None) # getattr(dataset, \"ood_loader_ambulance\", None)\n",
    "\n",
    "# Evaluate\n",
    "evaluate(\n",
    "    model,\n",
    "    test_loader,\n",
    "    args.dataset,\n",
    "    model_name=args.model,\n",
    "    ood_set=ood_loader,\n",
    "    ood_set_2=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning-shortcuts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
